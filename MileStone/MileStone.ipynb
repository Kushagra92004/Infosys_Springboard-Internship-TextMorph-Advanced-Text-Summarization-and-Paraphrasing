{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merged Notebook: Text Summarization, Paraphrasing, and Comparative Analysis\n",
        "\n",
        "This notebook combines text summarization, paraphrasing, and a comparative analysis of different transformer models. It is a merger of two separate notebooks, streamlined for clarity and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Session Setup & Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1.1: Install Required Libraries\n",
        "!pip install transformers sentencepiece sentence-transformers scikit-learn matplotlib pandas --quiet\n",
        "print(\"✅ All required libraries installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1.2: Import All Necessary Libraries\n",
        "# Core NLP libraries from Hugging Face\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Standard Python libraries\n",
        "import textwrap\n",
        "import re\n",
        "import requests\n",
        "import ssl\n",
        "\n",
        "# Handle SSL for file downloads if necessary\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "print(\"✅ All libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Model Loading\n",
        "Here, we load all the pre-trained models we'll need for summarization, paraphrasing, and similarity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 2.1: Load All Models and Tokenizers\n",
        "print(\"⏳ Loading Summarization models...\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "pegasus_sum_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "pegasus_sum_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "print(\"✅ Summarization models loaded: T5, BART, PEGASUS.\")\n",
        "\n",
        "print(\"\\n⏳ Loading Paraphrasing models...\")\n",
        "pegasus_para_model = PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\n",
        "pegasus_para_tokenizer = PegasusTokenizer.from_pretrained('tuner007/pegasus_paraphrase')\n",
        "\n",
        "paraphrase_t5_model = T5ForConditionalGeneration.from_pretrained('Vamsi/T5_Paraphrase_Paws')\n",
        "paraphrase_t5_tokenizer = T5Tokenizer.from_pretrained('Vamsi/T5_Paraphrase_Paws')\n",
        "\n",
        "paraphrase_bart_model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
        "paraphrase_bart_tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
        "print(\"✅ Paraphrasing models loaded: PEGASUS, T5-Paraphrase, BART-Paraphrase.\")\n",
        "\n",
        "print(\"\\n⏳ Loading Similarity model...\")\n",
        "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"✅ Similarity model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Core Functions (Our NLP Toolbox)\n",
        "This section contains all the core functions for performing summarization, paraphrasing, and calculating similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3.1: Summarization Functions\n",
        "\n",
        "def generate_t5_summary(text, min_len=40, max_len=150, beams=4):\n",
        "    \"\"\"Abstractive summarization with T5.\"\"\"\n",
        "    # T5 models require a prefix to know which task to perform.\n",
        "    input_text = \"summarize: \" + text.strip().replace(\"\\n\", \" \")\n",
        "    inputs = t5_tokenizer.encode(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "    summary_ids = t5_model.generate(\n",
        "        inputs, max_length=max_len, min_length=min_len, num_beams=beams,\n",
        "        no_repeat_ngram_size=3, length_penalty=2.0, early_stopping=True\n",
        "    )\n",
        "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_bart_summary(text, min_len=40, max_len=150, beams=4):\n",
        "    \"\"\"Abstractive summarization with BART.\"\"\"\n",
        "    inputs = bart_tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs, max_length=max_len, min_length=min_len, num_beams=beams,\n",
        "        no_repeat_ngram_size=3, length_penalty=2.0, early_stopping=True\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_pegasus_summary(text, min_len=40, max_len=150, beams=4):\n",
        "    \"\"\"Abstractive summarization with PEGASUS.\"\"\"\n",
        "    inputs = pegasus_sum_tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "    summary_ids = pegasus_sum_model.generate(\n",
        "        inputs, max_length=max_len, min_length=min_len, num_beams=beams,\n",
        "        no_repeat_ngram_size=3, length_penalty=2.0, early_stopping=True\n",
        "    )\n",
        "    return pegasus_sum_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3.2: Paraphrasing Functions\n",
        "\n",
        "def generate_pegasus_paraphrase(text, num_return=3, beams=10):\n",
        "    \"\"\"Generate paraphrases with PEGASUS.\"\"\"\n",
        "    inputs = pegasus_para_tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=60)\n",
        "    paraphrase_ids = pegasus_para_model.generate(\n",
        "        inputs, max_length=60, num_beams=beams,\n",
        "        num_return_sequences=num_return, early_stopping=True\n",
        "    )\n",
        "    return pegasus_para_tokenizer.batch_decode(paraphrase_ids, skip_special_tokens=True)\n",
        "\n",
        "def generate_t5_paraphrase(text, num_return=3, beams=10):\n",
        "    \"\"\"Generate paraphrases with T5-Paraphrase.\"\"\"\n",
        "    input_text = f\"paraphrase: {text} </s>\"\n",
        "    inputs = paraphrase_t5_tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    paraphrase_ids = paraphrase_t5_model.generate(\n",
        "        inputs, max_length=60, num_beams=beams,\n",
        "        num_return_sequences=num_return, early_stopping=True\n",
        "    )\n",
        "    return paraphrase_t5_tokenizer.batch_decode(paraphrase_ids, skip_special_tokens=True)\n",
        "\n",
        "def generate_bart_paraphrase(text, num_return=3, beams=10):\n",
        "    \"\"\"Generate paraphrases with BART-Paraphrase.\"\"\"\n",
        "    inputs = paraphrase_bart_tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=60)\n",
        "    paraphrase_ids = paraphrase_bart_model.generate(\n",
        "        inputs, max_length=60, num_beams=beams,\n",
        "        num_return_sequences=num_return, early_stopping=True\n",
        "    )\n",
        "    return paraphrase_bart_tokenizer.batch_decode(paraphrase_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 3.3: Helper & Analysis Functions\n",
        "\n",
        "def compute_similarity(text1, text2):\n",
        "    \"\"\"Compute cosine similarity between texts.\"\"\"\n",
        "    embeddings = similarity_model.encode([text1, text2])\n",
        "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove special characters and normalize whitespace.\"\"\"\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Task 1 - Text Summarization\n",
        "Let's start by using the T5 model to summarize different types of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4.1: Example 1 - Technical Text (Quantum Computing)\n",
        "technical_text = \"\"\"\n",
        "Quantum computing is a revolutionary type of computation that harnesses the collective properties of quantum states,\n",
        "such as superposition and entanglement, to perform calculations. While classical computers use bits that can be either\n",
        "a 0 or a 1, a quantum computer uses qubits, which can be a 0, a 1, or both at the same time. This fundamental difference\n",
        "allows quantum computers to solve complex problems that are intractable for even the most powerful classical supercomputers,\n",
        "with potential applications in cryptography, materials science, and drug discovery.\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_t5_summary(technical_text, min_len=25, max_len=50)\n",
        "\n",
        "print(\"----------- TECHNICAL TEXT -----------\")\n",
        "print(textwrap.fill(technical_text, width=100))\n",
        "print(\"\\n✨---------- T5 SUMMARY -----------✨\")\n",
        "print(textwrap.fill(summary, width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4.2: Example 2 - Business Text (Market Analysis)\n",
        "business_text = \"\"\"\n",
        "The global market for renewable energy is projected to experience robust growth over the next decade. Key drivers include\n",
        "increasing government incentives for clean energy, declining costs of solar and wind technologies, and growing consumer\n",
        "awareness regarding climate change. However, challenges remain, such as the intermittency of renewable sources and the\n",
        "need for significant grid infrastructure upgrades. Companies that can innovate in energy storage solutions and grid\n",
        "management are best positioned to capitalize on this market trend.\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_t5_summary(business_text, min_len=30, max_len=90)\n",
        "\n",
        "print(\"----------- BUSINESS TEXT -----------\")\n",
        "print(textwrap.fill(business_text, width=100))\n",
        "print(\"\\n✨---------- T5 SUMMARY -----------✨\")\n",
        "print(textwrap.fill(summary, width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4.3: Example 3 - Creative Text (Literary Description)\n",
        "creative_text = \"\"\"\n",
        "The ancient library was a labyrinth of shadows and whispered knowledge. Sunlight struggled through a high,\n",
        "arched window, illuminating motes of dust that danced like tiny sprites in the golden shafts of light.\n",
        "The air smelled of aging paper, leather, and a faint, sweet hint of vanilla. Every towering bookshelf\n",
        "was a gateway to another world, each leather-bound volume a silent promise of adventure, history,\n",
        "or forgotten magic. It was a place where time itself seemed to slow down, holding its breath in reverence\n",
        "for the stories it contained.\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_t5_summary(creative_text, min_len=20, max_len=70)\n",
        "\n",
        "print(\"----------- CREATIVE TEXT -----------\")\n",
        "print(textwrap.fill(creative_text, width=100))\n",
        "print(\"\\n✨---------- T5 SUMMARY -----------✨\")\n",
        "print(textwrap.fill(summary, width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4: Interactive Summarizer Studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Your Interactive T5 Summarizer Studio! ⚡️\n",
        "#@markdown ### 👈 Paste your text below and tune the parameters!\n",
        "input_text = 'The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy. As the largest optical telescope in space, its high resolution and sensitivity allow it to view objects too old, distant, or faint for the Hubble Space Telescope. This has enabled investigations in many fields of astronomy and cosmology, such as observation of the first stars, the formation of the first galaxies, and detailed atmospheric characterization of potentially habitable exoplanets. The U.S. National Aeronautics and Space Administration (NASA) led JWST\\'s development in collaboration with the European Space Agency (ESA) and the Canadian Space Agency (CSA).' #@param {type:\"string\"}\n",
        "min_length = 45 #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "max_length = 140 #@param {type:\"slider\", min:50, max:200, step:10}\n",
        "num_beams = 5 #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "\n",
        "# --- Run the summarizer with your settings ---\n",
        "generated_summary = generate_t5_summary(input_text, min_len=min_length, max_len=max_length, beams=num_beams)\n",
        "\n",
        "# --- Display the results and analysis ---\n",
        "original_word_count = len(input_text.split())\n",
        "summary_word_count = len(generated_summary.split())\n",
        "reduction = 100 - (summary_word_count / original_word_count * 100)\n",
        "\n",
        "print(\"----------- YOUR INPUT TEXT -----------\")\n",
        "print(textwrap.fill(input_text, width=100))\n",
        "print(\"\\n✨---------- GENERATED SUMMARY -----------✨\")\n",
        "print(textwrap.fill(generated_summary, width=100))\n",
        "print(\"\\n📊---------- ANALYSIS -----------📊\")\n",
        "print(f\"Original Word Count: {original_word_count}\")\n",
        "print(f\"Summary Word Count: {summary_word_count}\")\n",
        "print(f\"Text Reduction: {reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Task 2 - Text Paraphrasing\n",
        "Now we'll use the PEGASUS model to generate alternative phrasings for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5.1: Example 1 - Formal / Academic Sentence\n",
        "formal_sentence = \"The empirical data indicates a statistically significant correlation between the two variables.\"\n",
        "paraphrases = generate_pegasus_paraphrase(formal_sentence, num_return=4)\n",
        "\n",
        "print(f\"----------- ORIGINAL FORMAL SENTENCE -----------\\n'{formal_sentence}'\\n\")\n",
        "print(\"🤖---------- PEGASUS PARAPHRASES ----------🤖\")\n",
        "for i, p in enumerate(paraphrases):\n",
        "    print(f\"  {i+1}. {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5.2: Example 2 - Casual / Idiomatic Sentence\n",
        "casual_sentence = \"To be honest, that new project is a real pain in the neck.\"\n",
        "paraphrases = generate_pegasus_paraphrase(casual_sentence, num_return=4)\n",
        "\n",
        "print(f\"----------- ORIGINAL CASUAL SENTENCE -----------\\n'{casual_sentence}'\\n\")\n",
        "print(\"🤖---------- PEGASUS PARAPHRASES ----------🤖\")\n",
        "for i, p in enumerate(paraphrases):\n",
        "    print(f\"  {i+1}. {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 5.3: Example 3 - Marketing Call-to-Action\n",
        "marketing_sentence = \"Don't miss out on our exclusive offer – shop now to save 50%!\"\n",
        "paraphrases = generate_pegasus_paraphrase(marketing_sentence, num_return=4)\n",
        "\n",
        "print(f\"----------- ORIGINAL MARKETING SENTENCE -----------\\n'{marketing_sentence}'\\n\")\n",
        "print(\"🤖---------- PEGASUS PARAPHRASES ----------🤖\")\n",
        "for i, p in enumerate(paraphrases):\n",
        "    print(f\"  {i+1}. {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4: Interactive Paraphraser Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Your Interactive Paraphraser Playground! ⚡️\n",
        "#@markdown ### 👈 Type your sentence and choose your settings! (Uses PEGASUS)\n",
        "input_sentence = \"Learning new skills is essential for career growth.\" #@param {type:\"string\"}\n",
        "num_paraphrases = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "quality_vs_speed_beams = 9 #@param {type:\"slider\", min:2, max:15, step:1}\n",
        "\n",
        "# --- Run the paraphraser with your settings ---\n",
        "generated_paraphrases = generate_pegasus_paraphrase(input_sentence, num_return=num_paraphrases, beams=quality_vs_speed_beams)\n",
        "\n",
        "# --- Display the results ---\n",
        "print(f\"----------- ORIGINAL SENTENCE -----------\\n'{input_sentence}'\\n\")\n",
        "print(f\"🤖---------- {len(generated_paraphrases)} GENERATED PARAPHRASES (Quality: {quality_vs_speed_beams}) ----------🤖\")\n",
        "for i, p in enumerate(generated_paraphrases):\n",
        "    print(f\"  {i+1}. {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Task 3 - Comparative Model Analysis\n",
        "Now, let's compare the performance of different models (T5, BART, PEGASUS) on both summarization and paraphrasing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6.1: Define Enhanced Comparison Functions\n",
        "def compare_summarizers_enhanced(text, min_len=40, max_len=150, beams=4):\n",
        "    \"\"\"Enhanced comparison of T5, BART, and PEGASUS for summarization.\"\"\"\n",
        "    t5_sum = generate_t5_summary(text[:2000], min_len, max_len, beams)\n",
        "    bart_sum = generate_bart_summary(text[:2000], min_len, max_len, beams)\n",
        "    pegasus_sum = generate_pegasus_summary(text[:2000], min_len, max_len, beams)\n",
        "\n",
        "    metrics = {\n",
        "        't5': {\n",
        "            'summary': t5_sum,\n",
        "            'length': len(t5_sum.split()),\n",
        "            'sim_to_original': compute_similarity(t5_sum, text[:2000])\n",
        "        },\n",
        "        'bart': {\n",
        "            'summary': bart_sum,\n",
        "            'length': len(bart_sum.split()),\n",
        "            'sim_to_original': compute_similarity(bart_sum, text[:2000])\n",
        "        },\n",
        "        'pegasus': {\n",
        "            'summary': pegasus_sum,\n",
        "            'length': len(pegasus_sum.split()),\n",
        "            'sim_to_original': compute_similarity(pegasus_sum, text[:2000])\n",
        "        }\n",
        "    }\n",
        "    print(\"----------- Enhanced Summarization Comparison -----------\")\n",
        "    print(f\"T5 Summary (Length: {metrics['t5']['length']}, Sim to Original: {metrics['t5']['sim_to_original']:.3f}):\")\n",
        "    print(textwrap.fill(t5_sum, width=100))\n",
        "    print(f\"\\nBART Summary (Length: {metrics['bart']['length']}, Sim to Original: {metrics['bart']['sim_to_original']:.3f}):\")\n",
        "    print(textwrap.fill(bart_sum, width=100))\n",
        "    print(f\"\\nPEGASUS Summary (Length: {metrics['pegasus']['length']}, Sim to Original: {metrics['pegasus']['sim_to_original']:.3f}):\")\n",
        "    print(textwrap.fill(pegasus_sum, width=100))\n",
        "    return metrics\n",
        "\n",
        "def compare_paraphrasers_enhanced(text, num_return=3, beams=10):\n",
        "    \"\"\"Enhanced comparison of PEGASUS, T5-Paraphrase, and BART-Paraphrase.\"\"\"\n",
        "    peg_paras = generate_pegasus_paraphrase(text, num_return, beams)\n",
        "    t5_paras = generate_t5_paraphrase(text, num_return, beams)\n",
        "    bart_paras = generate_bart_paraphrase(text, num_return, beams)\n",
        "\n",
        "    metrics = {\n",
        "        'pegasus': {\n",
        "            'paraphrases': peg_paras,\n",
        "            'avg_length': sum(len(p.split()) for p in peg_paras) / len(peg_paras),\n",
        "            'avg_sim_to_original': sum(compute_similarity(p, text) for p in peg_paras) / len(peg_paras)\n",
        "        },\n",
        "        't5_paraphrase': {\n",
        "            'paraphrases': t5_paras,\n",
        "            'avg_length': sum(len(p.split()) for p in t5_paras) / len(t5_paras),\n",
        "            'avg_sim_to_original': sum(compute_similarity(p, text) for p in t5_paras) / len(t5_paras)\n",
        "        },\n",
        "        'bart_paraphrase': {\n",
        "            'paraphrases': bart_paras,\n",
        "            'avg_length': sum(len(p.split()) for p in bart_paras) / len(bart_paras),\n",
        "            'avg_sim_to_original': sum(compute_similarity(p, text) for p in bart_paras) / len(bart_paras)\n",
        "        }\n",
        "    }\n",
        "    print(\"----------- Enhanced Paraphrasing Comparison -----------\")\n",
        "    print(f\"PEGASUS Paraphrases (Avg Length: {metrics['pegasus']['avg_length']:.1f}, Avg Sim to Original: {metrics['pegasus']['avg_sim_to_original']:.3f}):\")\n",
        "    for i, p in enumerate(peg_paras, 1):\n",
        "        print(f\"  {i}. {p}\")\n",
        "    print(f\"\\nT5-Paraphrase Paraphrases (Avg Length: {metrics['t5_paraphrase']['avg_length']:.1f}, Avg Sim to Original: {metrics['t5_paraphrase']['avg_sim_to_original']:.3f}):\")\n",
        "    for i, p in enumerate(t5_paras, 1):\n",
        "        print(f\"  {i}. {p}\")\n",
        "    print(f\"\\nBART-Paraphrase Paraphrases (Avg Length: {metrics['bart_paraphrase']['avg_length']:.1f}, Avg Sim to Original: {metrics['bart_paraphrase']['avg_sim_to_original']:.3f}):\")\n",
        "    for i, p in enumerate(bart_paras, 1):\n",
        "        print(f\"  {i}. {p}\")\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6.2: Load Texts for Analysis from URLs\n",
        "def load_text_from_url(url):\n",
        "    \"\"\"Load and clean text from a URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "        # Simple cleaning for Gutenberg texts\n",
        "        start_idx = text.find(\"*** START OF\")\n",
        "        end_idx = text.find(\"*** END OF\")\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            text = text[start_idx:end_idx]\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "file_urls = {\n",
        "    \"Frankenstein\": \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
        "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "}\n",
        "\n",
        "loaded_texts = {}\n",
        "for name, url in file_urls.items():\n",
        "    print(f\"⏳ Loading '{name}'...\")\n",
        "    loaded_texts[name] = load_text_from_url(url)\n",
        "    print(f\"✅ Loaded '{name}' ({len(loaded_texts[name])} characters).\")\n",
        "\n",
        "frankenstein_text = loaded_texts[\"Frankenstein\"]\n",
        "pride_prejudice_text = loaded_texts[\"Pride and Prejudice\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6.3: Run Summarization Comparison on 'Frankenstein'\n",
        "print(\"----------- Summarization Comparison on 'Frankenstein'-----------\\n\")\n",
        "comparison_frankenstein = compare_summarizers_enhanced(frankenstein_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6.4: Run Summarization Comparison on 'Pride and Prejudice'\n",
        "print(\"----------- Summarization Comparison on 'Pride and Prejudice' -----------\\n\")\n",
        "comparison_pride = compare_summarizers_enhanced(pride_prejudice_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5: Visualize Comparison Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 6.5: Define Visualization Functions and Plot Results\n",
        "\n",
        "def plot_summarization_comparison(metrics, title=\"Summarization Comparison\"):\n",
        "    \"\"\"Plot enhanced metrics for summarization models.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    models = ['T5', 'BART', 'PEGASUS']\n",
        "    \n",
        "    # Length\n",
        "    lengths = [metrics['t5']['length'], metrics['bart']['length'], metrics['pegasus']['length']]\n",
        "    ax1.bar(models, lengths, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "    ax1.set_title('Summary Length (Word Count)')\n",
        "    ax1.set_ylabel('Words')\n",
        "\n",
        "    # Similarity to Original\n",
        "    similarities = [metrics['t5']['sim_to_original'], metrics['bart']['sim_to_original'], metrics['pegasus']['sim_to_original']]\n",
        "    ax2.bar(models, similarities, color=['#ffcc99','#c2c2f0','#ffb3e6'])\n",
        "    ax2.set_title('Semantic Similarity to Original Text')\n",
        "    ax2.set_ylabel('Cosine Similarity')\n",
        "    ax2.set_ylim(0, 1)\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def plot_paraphrasing_comparison(metrics, title=\"Paraphrasing Comparison\"):\n",
        "    \"\"\"Plot enhanced metrics for paraphrasing models.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    models = ['PEGASUS', 'T5', 'BART']\n",
        "    \n",
        "    # Average Length\n",
        "    avg_lengths = [metrics['pegasus']['avg_length'], metrics['t5_paraphrase']['avg_length'], metrics['bart_paraphrase']['avg_length']]\n",
        "    ax1.bar(models, avg_lengths, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "    ax1.set_title('Average Paraphrase Length (Words)')\n",
        "    ax1.set_ylabel('Words')\n",
        "\n",
        "    # Average Similarity to Original\n",
        "    avg_sims = [metrics['pegasus']['avg_sim_to_original'], metrics['t5_paraphrase']['avg_sim_to_original'], metrics['bart_paraphrase']['avg_sim_to_original']]\n",
        "    ax2.bar(models, avg_sims, color=['#ffcc99','#c2c2f0','#ffb3e6'])\n",
        "    ax2.set_title('Average Similarity to Original Sentence')\n",
        "    ax2.set_ylabel('Cosine Similarity')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# Plot the summarization results\n",
        "plot_summarization_comparison(comparison_frankenstein, title=\"Summarization Comparison (Frankenstein)\")\n",
        "plot_summarization_comparison(comparison_pride, title=\"Summarization Comparison (Pride and Prejudice)\")\n",
        "\n",
        "# Run and plot a paraphrasing comparison\n",
        "paraphrase_test_sentence = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        "paraphrase_metrics = compare_paraphrasers_enhanced(paraphrase_test_sentence)\n",
        "plot_paraphrasing_comparison(paraphrase_metrics, title=\"Paraphrasing Comparison (Pride & Prejudice Sentence)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Task 4 - Deeper Text Analysis\n",
        "Let's perform a simple bigram analysis to find the most common word pairs in our source texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 7.1: Bigram Analysis Function and Visualization\n",
        "def analyze_and_plot_bigrams(text, title=\"Top 5 Bigrams\"):\n",
        "    \"\"\"Compute and plot top bigrams from text.\"\"\"\n",
        "    text = clean_text(text[:5000])  # Clean and limit for performance\n",
        "    words = text.lower().split()\n",
        "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "    bigram_freq = Counter(bigrams).most_common(5)\n",
        "    \n",
        "    if bigram_freq:\n",
        "        labels, counts = zip(*[(f\"{w1} {w2}\", count) for (w1, w2), count in bigram_freq])\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.bar(labels, counts, color='teal')\n",
        "        plt.title(title, fontsize=14)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Could not generate bigrams for '{title}'.\")\n",
        "\n",
        "# Run analysis on our loaded texts\n",
        "analyze_and_plot_bigrams(frankenstein_text, title=\"Top 5 Bigrams in 'Frankenstein'\")\n",
        "analyze_and_plot_bigrams(pride_prejudice_text, title=\"Top 5 Bigrams in 'Pride and Prejudice'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Putting It All Together: The TextMorph Pipeline\n",
        "Finally, we can encapsulate all these steps into a simple, reusable pipeline class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 8.1: Define and Run the Full Pipeline\n",
        "class TextMorphPipeline:\n",
        "    \"\"\"Streamlined pipeline for text summarization, paraphrasing, and analysis.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reference_texts = []\n",
        "\n",
        "    def add_reference(self, text):\n",
        "        self.reference_texts.append(text)\n",
        "\n",
        "    def summarize(self, input_text, summarizer='t5', **kwargs):\n",
        "        print(f\"\\n--- Generating {summarizer.upper()} Summary ---\")\n",
        "        if summarizer == 't5':\n",
        "            return generate_t5_summary(input_text, **kwargs)\n",
        "        elif summarizer == 'bart':\n",
        "            return generate_bart_summary(input_text, **kwargs)\n",
        "        elif summarizer == 'pegasus':\n",
        "            return generate_pegasus_summary(input_text, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid summarizer. Choose 't5', 'bart', or 'pegasus'.\")\n",
        "\n",
        "    def paraphrase(self, input_text, paraphraser='pegasus', **kwargs):\n",
        "        print(f\"\\n--- Generating {paraphraser.upper()} Paraphrases ---\")\n",
        "        if paraphraser == 'pegasus':\n",
        "            return generate_pegasus_paraphrase(input_text, **kwargs)\n",
        "        elif paraphraser == 't5':\n",
        "            return generate_t5_paraphrase(input_text, **kwargs)\n",
        "        elif paraphraser == 'bart':\n",
        "            return generate_bart_paraphrase(input_text, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid paraphraser. Choose 'pegasus', 't5', or 'bart'.\")\n",
        "    \n",
        "    def analyze_similarity(self, input_text):\n",
        "        if self.reference_texts:\n",
        "            print(\"\\n--- Calculating Similarity to Reference Texts ---\")\n",
        "            for i, ref in enumerate(self.reference_texts):\n",
        "                similarity = compute_similarity(input_text, ref)\n",
        "                print(f\"Similarity to Reference {i+1}: {similarity:.3f}\")\n",
        "\n",
        "    def process(self, input_text, summarizer='t5', paraphrase_sentence=None):\n",
        "        print(f\"\\n{'='*20} PIPELINE RUN {'='*20}\")\n",
        "        summary = self.summarize(input_text, summarizer=summarizer, min_len=50, max_len=150)\n",
        "        print(textwrap.fill(summary, width=100))\n",
        "\n",
        "        if paraphrase_sentence:\n",
        "            paraphrases = self.paraphrase(paraphrase_sentence)\n",
        "            for i, p in enumerate(paraphrases):\n",
        "                print(f\"  {i+1}. {p}\")\n",
        "        \n",
        "        self.analyze_similarity(input_text)\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "# --- Initialize and run the pipeline ---\n",
        "pipeline = TextMorphPipeline()\n",
        "pipeline.add_reference(\"A story of creation, ambition, and tragedy.\")\n",
        "pipeline.add_reference(\"A social commentary on class and love in Regency England.\")\n",
        "\n",
        "# Process Frankenstein with a T5 summary\n",
        "pipeline.process(\n",
        "    frankenstein_text[:2000],\n",
        "    summarizer='t5',\n",
        "    paraphrase_sentence=\"It was on a dreary night of November that I beheld the accomplishment of my toils.\"\n",
        ")\n",
        "\n",
        "# Process Pride and Prejudice with a PEGASUS summary\n",
        "pipeline.process(\n",
        "    pride_prejudice_text[:2000],\n",
        "    summarizer='pegasus',\n",
        "    paraphrase_sentence=\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
